# === PostgreSQL ì—°ê²° + Qdrant ê¸°ë³¸ ë™ì‘ + search_corpusâ†’ì„ë² ë”©â†’Qdrant ì—…ì„œíŠ¸ E2E ìŠ¤ëª¨í¬ í…ŒìŠ¤íŠ¸ ===
from __future__ import annotations
from typing import List, Dict, Tuple, Optional

import psycopg2
from psycopg2.extras import RealDictCursor

from qdrant_client import models
from core.config import settings
from infra.qdrant import (
    client,
    initialize_qdrant,
    upsert_points,
    delete_collection,
)
from workers.embedder import embed_batch


# ------------------------------------------------------------
# í™˜ê²½/ì„¤ì •
# ------------------------------------------------------------
# ğŸ”¹ ë„¤ê°€ ì¤€ DB í•˜ë“œì½”ë”©
DB_CONFIG = {
    "dbname":   "pre_capstone",
    "user":     "pre_capstone",
    "password": "pre_capstone1234!",
    "host":     "34.50.13.135",
    "port":     "5432",
}

# ğŸ”¹ í…ŒìŠ¤íŠ¸ ì»¬ë ‰ì…˜ (ìš´ì˜ ì˜¤ì—¼ ë°©ì§€ìš©)
TEST_COLLECTION = f"{settings.QDRANT_COLLECTION}_smoke"

# ğŸ”¹ search_corpusì—ì„œ ê°€ì ¸ì˜¬ SQL (id/title/body/category/updated_at í•„ìˆ˜)
SQL_FETCH_ONE = """
    SELECT id, title, body, category, updated_at
    FROM public.search_corpus
    ORDER BY id
    LIMIT %s
"""

# ğŸ”¹ llm_outputs ì¡´ì¬ ê°€ì • (ì´ë¯¸ ìƒì„±ë˜ì–´ ìˆìŒ)
SQL_HAS_LLM = "SELECT 1 FROM public.llm_outputs WHERE source_id = %s"
SQL_PUT_LLM  = """
INSERT INTO public.llm_outputs (source_id, normalized, llm_version)
VALUES (%s, %s, %s)
ON CONFLICT (source_id) DO NOTHING
"""

KEEP_TEST_COLLECTION = False   # Trueë©´ ì»¬ë ‰ì…˜ ìœ ì§€, Falseë©´ ë§ˆì§€ë§‰ì— ì‚­ì œ


# ------------------------------------------------------------
# ìœ í‹¸
# ------------------------------------------------------------
def call_llm_normalize(title: str, body: str) -> Tuple[str, str]:
    """(ì„ì‹œ ìŠ¤í…) ìµœì´ˆ ì§ˆì˜ì¸ ê²½ìš° LLMì´ ê°€ê³µí•œ í…ìŠ¤íŠ¸ë¼ê³  ê°€ì •."""
    normalized = f"{title}\n{body}"
    llm_version = "stub-0"
    return normalized, llm_version

def choose_text_for_embedding(cur, row: Dict) -> Tuple[str, str, Optional[str]]:
    """
    PM ê·œì¹™ ë°˜ì˜:
        - ìµœì´ˆ ì§ˆì˜: llm_outputsì— ì—†ìœ¼ë©´ LLM í˜¸ì¶œ â†’ normalized ì €ì¥ â†’ ì´ë²ˆì—” LLM í…ìŠ¤íŠ¸ë¡œ ì„ë² ë”© (source='llm')
        - ì¬ì§ˆì˜: llm_outputsì— ìˆìœ¼ë©´ DB ì •ê·œí™” í…ìŠ¤íŠ¸ë¡œ ì„ë² ë”© (source='db')
    """
    cur.execute(SQL_HAS_LLM, (row["id"],))
    seen = cur.fetchone() is not None

    if seen:
        text = f"{row['title']}\n{row['body']}"
        return text, "db", None
    else:
        normalized, llm_ver = call_llm_normalize(row["title"], row["body"])
        cur.execute(SQL_PUT_LLM, (row["id"], normalized, llm_ver))
        return normalized, "llm", llm_ver


# ------------------------------------------------------------
# ì²´í¬/í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
# ------------------------------------------------------------
def check_postgres():
    print("[PG] ì—°ê²° ì‹œë„â€¦")
    conn = psycopg2.connect(**DB_CONFIG)
    cur = conn.cursor(cursor_factory=RealDictCursor)

    cur.execute("SELECT now() AS now_utc")
    print(f"[OK] postgres now() = {cur.fetchone()['now_utc']}")

    cur.execute("""
        SELECT table_schema, table_name
        FROM information_schema.tables
        WHERE table_schema = 'public'
        ORDER BY table_name
        LIMIT 10
    """)
    rows = cur.fetchall()
    names = [f"{r['table_schema']}.{r['table_name']}" for r in rows]
    print(f"[OK] public ìŠ¤í‚¤ë§ˆ í…Œì´ë¸” ì˜ˆì‹œ: {names}")

    cur.close()
    conn.close()


def check_qdrant_basic():
    print("[Qdrant] ì—°ê²°/ì»¬ë ‰ì…˜ ë³´ì¥â€¦")
    initialize_qdrant(TEST_COLLECTION)
    print("[OK] í…ŒìŠ¤íŠ¸ ì»¬ë ‰ì…˜ ì¤€ë¹„ ì™„ë£Œ")

    # ì—…ì„œíŠ¸ â†’ retrieve â†’ ì‚­ì œ (í•„í„° ì¸ë±ìŠ¤ ì—†ì´ë„ í†µê³¼)
    test_id = 999_999_999
    p = models.PointStruct(
        id=test_id,
        vector=[0.0] * 1024,
        payload={"smoke": True, "note": "connectivity-check"},
    )
    upsert_points([p], collection_name=TEST_COLLECTION)
    print("[OK] upsert 1ê±´")

    got = client.retrieve(
        collection_name=TEST_COLLECTION,
        ids=[test_id],
        with_payload=True,
        with_vectors=False,
    )
    assert len(got) == 1, "retrieve ê²°ê³¼ 0ê±´"
    assert got[0].payload.get("smoke") is True, "payload.smoke != True"
    print("[OK] retrieve í™•ì¸")

    client.delete(
        collection_name=TEST_COLLECTION,
        points_selector=models.PointIdsList(points=[test_id]),
    )
    print("[OK] cleanup (delete 1ê±´)")


def test_ingest_small_sample(limit: int = 5):
    """
    search_corpusì—ì„œ ì†ŒëŸ‰ ê°€ì ¸ì™€ ì„ë² ë”© í›„ TEST_COLLECTIONì— ì—…ì„œíŠ¸ â†’ í•˜ë‚˜ ì„ì˜ ì¡°íšŒ í™•ì¸.
    """
    print(f"[E2E] search_corpus â†’ ì„ë² ë”© â†’ Qdrant ì—…ì„œíŠ¸ (limit={limit})")

    # 1) í…ŒìŠ¤íŠ¸ ì»¬ë ‰ì…˜ ë³´ì¥
    initialize_qdrant(TEST_COLLECTION)

    # 2) PG ì—°ê²°/ì¡°íšŒ
    conn = psycopg2.connect(**DB_CONFIG)
    cur = conn.cursor(cursor_factory=RealDictCursor)
    print("[PG] ì—°ê²° ì„±ê³µ")

    cur.execute(SQL_FETCH_ONE, (limit,))
    rows = cur.fetchall()
    assert rows, "search_corpusì—ì„œ ë ˆì½”ë“œë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŒ"
    print(f"[PG] fetched {len(rows)} rows")

    # 3) PM ê·œì¹™ì— ë”°ë¼ ì„ë² ë”© ì†ŒìŠ¤ í…ìŠ¤íŠ¸/ë©”íƒ€ ì •ë¦¬
    texts: List[str] = []
    metas: List[Dict] = []
    for r in rows:
        text, source_flag, llm_ver = choose_text_for_embedding(cur, r)
        conn.commit()  # llm_outputs INSERT ë°˜ì˜

        metas.append({
            "id": int(r["id"]),
            "title": r["title"],
            "category": r.get("category"),
            "updated_at": str(r.get("updated_at")),
            "source": source_flag,
            "llm_version": llm_ver,
            "embedding_version": "kure-v1",
        })
        texts.append(text)

    # 4) ì„ë² ë”©
    vecs = embed_batch(texts)

    # 5) Qdrant í¬ì¸íŠ¸ êµ¬ì„±
    points: List[models.PointStruct] = []
    for meta, vec in zip(metas, vecs):
        points.append(
            models.PointStruct(
                id=meta["id"],
                vector=vec,
                payload={
                    "pg_id": meta["id"],
                    "title": meta["title"],
                    "category": meta["category"],
                    "updated_at": meta["updated_at"],
                    "source": meta["source"],
                    "llm_version": meta["llm_version"],
                    "embedding_version": meta["embedding_version"],
                },
            )
        )

    # 6) ì—…ì„œíŠ¸
    upsert_points(points, collection_name=TEST_COLLECTION)
    print(f"[OK] upsert {len(points)}ê±´")

    # 7) ì¹´ìš´íŠ¸/ìƒ˜í”Œ ì¡°íšŒ í™•ì¸
    total_cnt = client.count(TEST_COLLECTION, exact=True).count
    print(f"[OK] í…ŒìŠ¤íŠ¸ ì»¬ë ‰ì…˜ ì „ì²´ ê°œìˆ˜: {total_cnt}")

    sample_id = metas[0]["id"]
    got = client.retrieve(
        collection_name=TEST_COLLECTION,
        ids=[sample_id],
        with_payload=True,
        with_vectors=False,
    )
    assert len(got) == 1, f"retrieve ì‹¤íŒ¨ (id={sample_id})"
    payload = got[0].payload or {}
    print(f"[OK] ìƒ˜í”Œ(payload): id={sample_id}, source={payload.get('source')}, title={payload.get('title')!r}")

    cur.close()
    conn.close()
    print("[PG] ì—°ê²° ì¢…ë£Œ")


# ------------------------------------------------------------
# main
# ------------------------------------------------------------
if __name__ == "__main__":
    print("Settings loaded successfully!")
    print(f"Qdrant URL: {settings.QDRANT_URL}")
    print("=== SMOKE TESTS START ===")
    try:
        check_postgres()            # 1) PG ë¶™ëŠ”ì§€/í…Œì´ë¸” ë³´ì´ëŠ”ì§€
        check_qdrant_basic()        # 2) Qdrantì— ì“°ê¸°/ì½ê¸°/ì‚­ì œ ë™ì‘ í™•ì¸(í…ŒìŠ¤íŠ¸ ì»¬ë ‰ì…˜)
        test_ingest_small_sample(5) # 3) ì‹¤ì œ search_corpusâ†’ì„ë² ë”©â†’Qdrant ì—…ì„œíŠ¸ í™•ì¸(5ê±´)

        print("=== SMOKE TESTS OK ===")
    finally:
        if not KEEP_TEST_COLLECTION:
            print(f"[CLEANUP] í…ŒìŠ¤íŠ¸ ì»¬ë ‰ì…˜ ì‚­ì œ: {TEST_COLLECTION}")
            delete_collection(TEST_COLLECTION)
